# OpenAI Local Proxy Environment Configuration
# Copy this file to .env and customize as needed

# ============================================
# Backend Configuration
# ============================================

# URL of the LM Studio backend
BACKEND_URL=http://10.50.10.14:1234

# Backend request timeout in seconds
BACKEND_TIMEOUT=300

# ============================================
# Server Configuration
# ============================================

# Host to bind to (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
SERVER_HOST=0.0.0.0

# Port to listen on
SERVER_PORT=8080

# Number of worker processes (1 for development, 2-4 for production)
SERVER_WORKERS=1

# ============================================
# Logging Configuration
# ============================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: text or json
LOG_FORMAT=text

# Include request bodies in logs (WARNING: may log sensitive data)
LOG_REQUEST_BODY=false

# Include response bodies in logs (WARNING: may log sensitive data)
LOG_RESPONSE_BODY=false

# ============================================
# Authentication Configuration
# ============================================

# Enable API key authentication
AUTH_ENABLED=false

# Comma-separated list of valid API keys (only used if AUTH_ENABLED=true)
# Example: AUTH_VALID_KEYS=sk-local-key-1,sk-local-key-2
AUTH_VALID_KEYS=

# ============================================
# Model Configuration
# ============================================

# Default model to use when none is specified
DEFAULT_MODEL=llama-3.1-instruct-13b

# Model mappings (format: OPENAI_MODEL:LOCAL_MODEL,...)
# Example: MODEL_MAPPINGS=gpt-3.5-turbo:llama-3.1,gpt-4:llama-3.1
MODEL_MAPPINGS=

# ============================================
# CORS Configuration
# ============================================

# Enable CORS support
CORS_ENABLED=true

# Comma-separated list of allowed origins (* for all)
CORS_ORIGINS=*

# ============================================
# Feature Flags
# ============================================

# Enable streaming support
ENABLE_STREAMING=true

# Enable embeddings endpoint
ENABLE_EMBEDDINGS=true

# Enable Prometheus metrics endpoint
ENABLE_METRICS=false

# ============================================
# Performance Configuration
# ============================================

# Maximum request body size in bytes (10MB default)
MAX_REQUEST_SIZE=10485760

# Default timeout for requests in seconds
REQUEST_TIMEOUT=300

# Maximum number of concurrent connections
MAX_CONNECTIONS=100

# Connection pool keep-alive connections
KEEP_ALIVE_CONNECTIONS=10

# ============================================
# Development/Debug Configuration
# ============================================

# Enable auto-reload on code changes (development only)
RELOAD=false

# Enable debug mode (shows detailed errors)
DEBUG=false
